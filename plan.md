<!-- user logs in.
gets brief about mission 
    -> api call to generate story and task and keep stuff ready (asynchronously in mongodb with status=processing till finished, then status = completed). 
        -> return the brief about task

executes command to join into fm freq (let's say 93.5) 
    -> api call recieved. poll mongodb every sec till status changes from processing to finished.
        -> connect websocket
        
websocket connected
    -> send _name_ of each caller.
    -> continuously audio from user's mic is streamed to python backend in chunks
    keep doing this till timelimit:
        parallel task 1:
            -> generate 10 dialogues at once (or less if waiting for user's response) from gemini (use good system prompt, pass story and past dialogues as context)
            -> decide voice actors for each speaker. give _name_ to each.
            -> send json list with name and audio of each dialogue. gaps will be a segment too.
            -> stream the tts audio in segments to user if possible
        parallel task 2:
            -> use vad to check if user is speaking or interrupting.
            -> if they are:
                -> pause the audio stream being sent to user. also mark the context till where the dialogue was done and add a tag like "(user interrupted)".
                -> use user's audio chunks to do stt (speech to text). then when user is done, append the stt text to context and continue parallel task 1 with this context
            -> else:
                -> continue -->

with push to talk: (this is what we are doing)
    user logs in (handled by nextjs).
    gets brief about mission by:
        -> Making a POST request to `/api/v1/create_mission`.
        -> The API synchronously generates the initial mission details (story, characters, etc.) and saves it to the database with `status: "stage1"`.
        -> The API immediately returns this initial mission data.
        -> In the background, the server starts generating a unified dialogue prompt for the next phase, and updates the mission status to `status: "stage2"` when complete.

    executes command to join into fm freq (let's say 93.5)
        -> The client polls the `GET /api/v1/mission_status/{mission_id}` endpoint until the `status` field becomes `"stage2"`.
        -> Once the status is `"stage2"`, the client has the dialogue prompt and can connect to the WebSocket for the interactive phase.

    websocket connected
        -> send _name_ of each caller. (no need as already stored in mongo db)
        -> main loop until timelimit:
            -> if current dialogue segment is a pause for user to speak:
                -> wait up to 10 seconds for user to press and hold the "Talk" button.
                -> if user presses button:
                    -> while button is pressed, record and stream user's mic audio to python backend in chunks.
                    -> on button release, stop recording and send final audio chunk.
                    -> backend performs STT on received audio.
                        -> if STT detects that audio is blank, add "user spoke nothing" to context (same as else above).
                        -> else, append transcribed text to context.
                -> else (user does not speak within 10 seconds):
                    -> add "user spoke nothing" to context.
            -> generate next set of dialogues from gemini (use good system prompt, pass story and past dialogues as context).
            -> decide voice actors for each speaker. give _name_ to each.
            -> send json list with name and audio of each dialogue. gaps will be a segment too.
            -> stream the tts audio in segments to user if possible.
        -> repeat for each dialogue segment until timelimit.

## Technology Stack & Core Mechanics

*   **Web Framework**: FastAPI will be used for the backend API. It's modern, fast, and has great support for Pydantic models which we'll use for structured data.
*   **Language Model (LLM)**: We are using Google's `gemini-2.5-flash` model via the `google-genai` Python library for all content generation tasks, including the initial mission. For dialogue prompts we'll use `gemini-2.5-flash-lite-preview-06-17` for fastest response.
*   **Text-to-Speech (TTS) & Speech-to-Text (STT)**: We will use the `deepgram-sdk` for Python for all speech synthesis and recognition tasks.
*   **Database**: MongoDB will be used for storing mission data, status, and context.

### Game Mechanics

*   **Listeners**: When a mission starts, Gemini will generate the initial number of listeners for the radio show.
*   **Awakened Listeners**: The game will track the number of "awakened" listeners, starting at 0.
*   **Awakening Metric**: The user's performance in conversations will determine how many listeners are "awakened". This metric will be generated by Gemini alongside the dialogue responses. We will use Pydantic models to ensure Gemini provides this data in a structured format (e.g., `{"dialogue": "...", "awakened_listeners_increase": 5}`).
# Backend Implementation Plan

This document outlines the plan for implementing the Python backend for the Radio Mirchi hackathon project.

## Phase 1: Initial Setup and First Endpoint

### 1. Project Structure

We will organize the project with a clear and scalable structure.

```
.
├── app
│   ├── __init__.py
│   ├── main.py             # FastAPI app initialization
│   ├── api
│   │   ├── __init__.py
│   │   └── v1
│   │       ├── __init__.py
│   │       └── endpoints
│   │           ├── __init__.py
│   │           └── propaganda.py   # Endpoint for /create_propaganda
│   ├── core
│   │   ├── __init__.py
│   │   └── config.py         # Configuration settings
│   ├── schemas
│   │   ├── __init__.py
│   │   └── propaganda.py   # Pydantic models for propaganda
│   └── services
│       ├── __init__.py
│       └── llm_service.py    # Service for Gemini integration
├── .gitignore
├── docker-compose.yml
├── Dockerfile
├── env.example
├── plan.md
├── implementation_plan.md
└── requirements.txt
```

### 2. Dependencies

We will add the required Python libraries to `requirements.txt`:

*   `fastapi`
*   `uvicorn`
*   `pydantic`
*   `google-generativeai`
*   `python-dotenv`
*   `deepgram-sdk`

### 3. Configuration

A `core/config.py` file will manage environment variables (like API keys) using `python-dotenv`.

### 4. Pydantic Schemas

In `schemas/propaganda.py`, we will define the Pydantic models for the `/create_propaganda` endpoint's response.

```python
from pydantic import BaseModel, Field
from typing import List

class Speaker(BaseModel):
    name: str
    gender: str

class Propaganda(BaseModel):
    summary: str
    full_script: str # The full propaganda script generated by the LLM
    speakers: List[Speaker] = Field(..., min_items=1, max_items=4)
    initial_listeners: int
    awakened_listeners: int = 0
```

### 5. LLM Service

The `services/llm_service.py` will contain the logic for interacting with the Gemini API. It will have a function that takes a topic and generates the propaganda details, returning a structured `Propaganda` object. We will use a detailed prompt to ensure Gemini returns the data in the desired JSON format that can be parsed into our Pydantic model.

### 6. API Endpoint: `/create_propaganda`

The first endpoint will be implemented in `api/v1/endpoints/propaganda.py`.

*   **Route**: `/api/v1/create_propaganda`
*   **Method**: `POST`
*   **Request Body**: A simple model, perhaps `{"topic": "string"}`.
*   **Response Body**: The `Propaganda` schema defined above.

This endpoint will:
1.  Receive a topic for the propaganda.
2.  Call the `llm_service` to generate the propaganda script, summary, speakers, and listener count.
3.  Return the generated data as a JSON response.

### 7. FastAPI Application

The main FastAPI application will be set up in `app/main.py`. This file will import and include the router from the `propaganda.py` endpoint file.